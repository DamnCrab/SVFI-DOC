import{_ as h,a as c,b as p,c as u,d as m,e as f,f as g,g as b}from"./66-VHjAiVOU.js";import{_ as v}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as l,o as y,c as w,b as e,d as t,a as i,w as d,e as n}from"./app-uOPc2Bvs.js";const _="/Statics/en/UserGuide/adv-workflow-recovery.png",x="/Statics/en/UserGuide/restore-example.png",k="/Statics/en/UserGuide/scene-detection.png",S="/Statics/en/UserGuide/scene-detection-tc.png",T="/Statics/en/UserGuide/resolution-settings.png",C="/Statics/en/UserGuide/sr-settings.png",I="/Statics/en/UserGuide/inpaint_1.png",A="/Statics/en/UserGuide/inpaint_2.png",R="/Statics/en/UserGuide/inpaint_3.png",q="/Statics/en/UserGuide/inpaint_4.png",F="/Statics/en/UserGuide/encode-settings.png",P="/Statics/en/UserGuide/io-control.png",U="/Statics/en/UserGuide/left-title-bar.png",D={},V=n('<p>The following content will introduce you to the advanced settings part of the software</p><h1 id="basic-task-settings" tabindex="-1"><a class="header-anchor" href="#basic-task-settings" aria-hidden="true">#</a> Basic Task Settings</h1><h2 id="workflow-recovery" tabindex="-1"><a class="header-anchor" href="#workflow-recovery" aria-hidden="true">#</a> Workflow Recovery</h2><div align="center"><img src="'+_+'" width="600"></div><h3 id="automatic-configure" tabindex="-1"><a class="header-anchor" href="#automatic-configure" aria-hidden="true">#</a> Automatic Configure</h3><div class="hint-container tip"><p class="hint-container-title">Tips</p><ul><li>When encountering a situation where the task is interrupted due to power outage or other unexpected situations that terminate the task and cause the program to exit, you can restore the last block position by clicking on <strong>Automatic Progress Search</strong>.</li><li>You can also directly drag the project folder into the software, and the software will automatically search for the progress corresponding to the project folder.</li></ul></div><p>Please click on the task entry to restore the progress first before clicking this button. Then click &quot;<strong>Interpolate</strong>&quot;, and the software will pop up a window to confirm the starting position of frame filling for you.</p><h3 id="start-point-and-end-point" tabindex="-1"><a class="header-anchor" href="#start-point-and-end-point" aria-hidden="true">#</a> Start point and End point</h3><p>You can select the time period that needs processing</p><blockquote><p>Input format: <strong>hours:minutes:seconds</strong></p></blockquote><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>After specifying the start and end frame filling moments, manual termination or power outage may lead to the failure of progress restoration</p></div><h3 id="start-block-count-and-start-input-frame-count" tabindex="-1"><a class="header-anchor" href="#start-block-count-and-start-input-frame-count" aria-hidden="true">#</a> Start Block Count and Start Input Frame Count</h3><p>Used when automatic progress search fails or when it is necessary to manually specify the starting position of frame filling, which can be used to manually restore the progress of frame filling.</p><ul><li>Start Block Count = The <strong>last chunk number exported in the output folder + 1</strong> (for example, in the figure, chunk-001, the start block count should be <strong>1+1=2</strong>).</li><li>Start Input Frame Count = <code>Single output block size in the output quality setting (rendering setting) * (Start Block Count - 1)</code>.</li></ul><div align="center"><img src="'+x+'" width="600"></div><p>As shown in the above figure, a video chunk has 1000 frames.</p><h3 id="restore-to-origin" tabindex="-1"><a class="header-anchor" href="#restore-to-origin" aria-hidden="true">#</a> Restore to Origin</h3><p>Set the start block and start input frame count to the system default value, and <strong>the software will automatically search for the restoration point and restore the task progress</strong>.</p><div align="center"><img src="'+c+'" width="600"></div><h3 id="risk-mode" tabindex="-1"><a class="header-anchor" href="#risk-mode" aria-hidden="true">#</a> Risk Mode</h3><p>When it is necessary to restore the task progress, enabling this item can <strong>reduce the time required for the program to restore the progress</strong>, but enabling it may cause <strong>audio-video out of sync</strong>.</p><p><strong>Not recommended to enable</strong>.</p><h1 id="advanced-software-settings" tabindex="-1"><a class="header-anchor" href="#advanced-software-settings" aria-hidden="true">#</a> Advanced software settings</h1><h2 id="transition-recognition" tabindex="-1"><a class="header-anchor" href="#transition-recognition" aria-hidden="true">#</a> Transition recognition</h2><div align="center"><img src="'+k+'" width="600"></div><h3 id="enable-transition-recognition" tabindex="-1"><a class="header-anchor" href="#enable-transition-recognition" aria-hidden="true">#</a> Enable transition recognition</h3><p>Identify scene switches</p><p>To avoid the <strong>jelly effect</strong> when switching scenes during frame filling, it is recommended to enable transition recognition.</p><p>After checking the Enable Transition Recognition option, the default value for the parameter below is usually 12; if you find that the final exported video is rather choppy, you can consider adjusting it to 15; if you find that there is a significant amount of <strong>jelly effect</strong>, you can consider adjusting the parameter value to 9, and the range of the parameter value is typically between 9 and 15.</p><p><strong>As shown in the picture: jelly produced by missed judgment in transition</strong></p><div align="center"><img src="'+p+'" width="600"></div><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>Due to the fact that this transition recognition mechanism is designed based on <strong>long video input</strong>, for some short video input (2-3 seconds), it is recommended to disable this transition recognition function or use third-party software to generate transition data for manual transition processing to avoid jitter caused by poor automatic recognition performance.</p></div><h3 id="maximum-recognition-threshold-default-does-not-need-to-be-adjusted" tabindex="-1"><a class="header-anchor" href="#maximum-recognition-threshold-default-does-not-need-to-be-adjusted" aria-hidden="true">#</a> Maximum recognition threshold (default does not need to be adjusted)</h3><p>When <strong>using a fixed transition recognition</strong> is not enabled (default), the recommended value for this option is 80-90.</p><p>When <strong>using a fixed transition recognition</strong> is enabled, the recommended value for this option is 40-60.</p><h3 id="use-a-fixed-transition-recognition" tabindex="-1"><a class="header-anchor" href="#use-a-fixed-transition-recognition" aria-hidden="true">#</a> Use a fixed transition recognition</h3><p>Use a fixed threshold (maximum recognition threshold) to identify transitions (unstable), at which point the software will perform a similarity detection on every two frames of the video.</p><p>If the similarity is greater than the threshold, it is considered a transition frame. This mode is prone to false positives or missed detections, and is only recommended to be used when there are many missed detections with the default transition detection method, such as in a mashup with a large number of lenses.</p><h2 id="manual-selection-of-transition-support" tabindex="-1"><a class="header-anchor" href="#manual-selection-of-transition-support" aria-hidden="true">#</a> Manual selection of transition support</h2><p>Display when the &quot;Transition Recognition&quot; button is turned off</p>',40),N=e("code",null,"transition list file",-1),E={href:"https://youtu.be/opig4Ur_mxM?si=P85QMhrOn09r44Hb",target:"_blank",rel:"noopener noreferrer"},G=n('<p>This method enables the import of transitions manually marked using TC software to replace the automatically recognized transitions, providing full control over where to fill frames and where not to in the input video.</p><div align="center"><img src="'+S+'" width="600"></div><h2 id="other-transition-detection-settings" tabindex="-1"><a class="header-anchor" href="#other-transition-detection-settings" aria-hidden="true">#</a> Other transition detection settings</h2><h3 id="output-transition-frame" tabindex="-1"><a class="header-anchor" href="#output-transition-frame" aria-hidden="true">#</a> Output transition frame</h3><p>Output the transition frame in the video.</p><p>The transition frame will be accompanied by relevant decision information and output in the scene folder of the project folder in png format. The project folder will be retained.</p><h3 id="transition-uses-frame-blending" tabindex="-1"><a class="header-anchor" href="#transition-uses-frame-blending" aria-hidden="true">#</a> Transition uses frame blending</h3><p>The traditional method is to copy the previous frame as the transition frame. This method is to blend the previous and next frames (gradually) to reduce the jitter caused by copying the transition frame.</p><h3 id="transition-interpolation" tabindex="-1"><a class="header-anchor" href="#transition-interpolation" aria-hidden="true">#</a> Transition interpolation</h3><p>The traditional method is to copy the previous frame as the transition frame. This method uses the first two frames of the transition frame and uses the frame filling algorithm for interpolation to reduce the jitter caused by copying the transition frame.</p><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>It is not recommended to enable this option when using the RIFE algorithm, otherwise jelly will be introduced.</p></div><h2 id="output-resolution-settings" tabindex="-1"><a class="header-anchor" href="#output-resolution-settings" aria-hidden="true">#</a> Output Resolution Settings</h2><div align="center"><img src="'+T+'" width="600"></div><h3 id="output-file-resolution" tabindex="-1"><a class="header-anchor" href="#output-file-resolution" aria-hidden="true">#</a> Output File Resolution</h3><p>The dropdown box is used for resolution preset selection.</p><p>When the preset is Custom (user-defined), you can set the final output resolution of the video. SVFI will adjust the resolution of the picture first, and then perform frame filling.</p><h3 id="crop-black-bars" tabindex="-1"><a class="header-anchor" href="#crop-black-bars" aria-hidden="true">#</a> Crop Black Bars</h3><p>Can be used to crop the black bars in the video, and the width and height need to be specified manually.</p>',18),M=e("strong",null,"height",-1),z=e("code",null,"270 = (original height - actual height) ÷ 2",-1),W=n('<blockquote><p>Example: Input video 1920x1080, actual resolution 1920x800, super-resolution 2x output 3840x1600. Then the black border height is filled in as 280, and the output resolution can be customized to 3840x1600</p></blockquote><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>If both width and height are entered as -1, SVFI will automatically identify the black bars of the input video and crop them.</p></div><h3 id="fill-in-black-bars-after-processing" tabindex="-1"><a class="header-anchor" href="#fill-in-black-bars-after-processing" aria-hidden="true">#</a> Fill in Black Bars After Processing</h3><p>After cropping the black bars, perform processing (frame filling or super-resolution), and automatically add the black bars back after filling the frames.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>This can reduce the amount of computing per frame to some extent and speed up the processing.</p></div><h2 id="use-ai-super-resolution-to-make-video-images-clearer" tabindex="-1"><a class="header-anchor" href="#use-ai-super-resolution-to-make-video-images-clearer" aria-hidden="true">#</a> Use AI Super-Resolution - to Make Video Images Clearer</h2>',6),O={class:"hint-container tip"},j=e("p",{class:"hint-container-title"},"Tips",-1),L={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},H=n('<div class="hint-container warning"><p class="hint-container-title">Warning</p><p>Performing frame filling and super-resolution simultaneously will consume more video memory, and insufficient video memory may cause the task to fail.</p><p>If the video memory is less than 10G, it is recommended to use Encode to complete the super-resolution first, and then perform frame filling.</p></div><h3 id="perform-super-resolution-after-frame-filling" tabindex="-1"><a class="header-anchor" href="#perform-super-resolution-after-frame-filling" aria-hidden="true">#</a> Perform Super-Resolution After Frame Filling</h3><p>Perform frame filling first, and then perform super-resolution (this usually slows down the speed, but reduces the video memory usage and often achieves better results).</p><h3 id="load-the-graphics-card" tabindex="-1"><a class="header-anchor" href="#load-the-graphics-card" aria-hidden="true">#</a> Load the Graphics Card</h3><p>Specify which graphics card to use for super-resolution.</p><h2 id="super-resolution-algorithm" tabindex="-1"><a class="header-anchor" href="#super-resolution-algorithm" aria-hidden="true">#</a> Super-Resolution Algorithm</h2><p>Currently, SVFI supports the following super-resolution algorithms.</p><table><thead><tr><th style="text-align:center;">Algorithm Name</th><th style="text-align:center;">Applicable Materials</th><th style="text-align:center;">Requires BETA</th><th style="text-align:center;">Available on AMD GPUs</th></tr></thead><tbody><tr><td style="text-align:center;">Anime4K</td><td style="text-align:center;">Anime</td><td style="text-align:center;"></td><td style="text-align:center;">√</td></tr><tr><td style="text-align:center;">AnimeSR</td><td style="text-align:center;">Anime</td><td style="text-align:center;"></td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">realCUGAN</td><td style="text-align:center;">Anime</td><td style="text-align:center;"></td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">ncnnCugan</td><td style="text-align:center;">Anime</td><td style="text-align:center;"></td><td style="text-align:center;">√</td></tr><tr><td style="text-align:center;">waifuCuda</td><td style="text-align:center;">Anime</td><td style="text-align:center;"></td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">PureBasicVSR</td><td style="text-align:center;">Live Action</td><td style="text-align:center;"></td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">BasicVSR++ T3</td><td style="text-align:center;">Live Action</td><td style="text-align:center;">√</td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">ATD</td><td style="text-align:center;">Live Action</td><td style="text-align:center;">√</td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">realESR</td><td style="text-align:center;">General</td><td style="text-align:center;"></td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">ncnnRealESR</td><td style="text-align:center;">General</td><td style="text-align:center;"></td><td style="text-align:center;">√</td></tr><tr><td style="text-align:center;">waifu2x</td><td style="text-align:center;">General</td><td style="text-align:center;"></td><td style="text-align:center;">√</td></tr><tr><td style="text-align:center;">TensorRT(ONNX)</td><td style="text-align:center;">General</td><td style="text-align:center;"></td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">Compact</td><td style="text-align:center;">General</td><td style="text-align:center;">√</td><td style="text-align:center;">×</td></tr><tr><td style="text-align:center;">SPAN</td><td style="text-align:center;">General</td><td style="text-align:center;">√</td><td style="text-align:center;">×</td></tr></tbody></table><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>SVFI defines the distinction between anime materials and live-action materials as follows:</p><p><strong>Anime</strong> materials are moving video clips mainly composed of flat image layers, and <strong>the boundaries between each layer and the other layers are clear</strong>. For example, hand-drawn 2D animation, most three-dimensional rendered two-dimensional pictures, etc.</p><p><strong>Live-action</strong> materials are real-world pictures or computer-generated pictures captured using a single-view camera, and <strong>the individual layers and their boundaries cannot be distinguished by the naked eye</strong>. For example, live-action movies, 3D CG, 3D game pictures, etc.</p><p>In particular, we consider animations made with 3D/3G backgrounds + 2D characters to be in the anime material category.</p></div><div align="center"><img src="'+C+'" width="600"></div><h2 id="introduction-to-the-super-resolution-model" tabindex="-1"><a class="header-anchor" href="#introduction-to-the-super-resolution-model" aria-hidden="true">#</a> Introduction to the Super-Resolution Model</h2><h3 id="realcugan" tabindex="-1"><a class="header-anchor" href="#realcugan" aria-hidden="true">#</a> realCUGAN</h3><p><strong>Exclusive for anime, the effect is very excellent</strong></p>',13),B=e("li",null,"up2x represents a 2x magnification, and 3x, 4x, etc. are similar.",-1),K={href:"https://github.com/bilibili/ailab/tree/main/Real-CUGAN",target:"_blank",rel:"noopener noreferrer"},Q=e("li",null,'Models with the word "conservative" are conservative models.',-1),Y=e("li",null,'Models with "no-denoise" do not perform noise reduction.',-1),J=e("li",null,'Models with "denoise" perform noise reduction, and the number behind represents the noise reduction intensity.',-1),X=n(`<h3 id="ncnncugan" tabindex="-1"><a class="header-anchor" href="#ncnncugan" aria-hidden="true">#</a> ncnnCUGAN</h3><p>The NCNN version of CUGAN (universal for AMD GPUs, NVIDIA GPUs, and I cards), the introduction is the same as above.</p><h3 id="waifucuda" tabindex="-1"><a class="header-anchor" href="#waifucuda" aria-hidden="true">#</a> waifuCuda</h3><p>Used for anime super-resolution, the speed and effect are somewhat similar to cugan.</p><h3 id="realesr" tabindex="-1"><a class="header-anchor" href="#realesr" aria-hidden="true">#</a> realESR</h3><p><strong>Applicable to both 3D anime, more suitable for anime</strong></p><ul><li>The RealESRGAN model tends to fill in the blanks, making the picture clearer and more vivid.</li><li>The RealESRNet model tends to smudge, but the picture retains its original color.</li><li>Models marked with &quot;anime&quot; are dedicated for anime super-resolution, and the speed is slightly faster than the previous two.</li><li>anime is the official model, and anime_110k is a self-trained model.</li><li>RealESR_RFDN is a self-trained super-resolution model with fast speed and is suitable for anime input.</li></ul><h3 id="ncnnrealesr" tabindex="-1"><a class="header-anchor" href="#ncnnrealesr" aria-hidden="true">#</a> ncnnRealESR</h3><p>The NCNN version of realESR, universal for AMD GPUs, I cards, and NVIDIA GPUs.</p><ul><li>realesr-animevideov3 (a relatively conservative anime video super-resolution model, with fast speed and high quality)</li><li>realesrgan-4xplus (4x magnification model)</li><li>realesrgan-4xplus-anime (4x anime magnification model)</li></ul><h3 id="animesr" tabindex="-1"><a class="header-anchor" href="#animesr" aria-hidden="true">#</a> AnimeSR</h3><p>An anime super-resolution algorithm developed by Tencent ARC Lab</p><p>Only one 4x magnification model (AnimeSR_v2_x4.pth), the effect is more conservative compared to cugan.</p><h3 id="basicvsrplusplusrestore" tabindex="-1"><a class="header-anchor" href="#basicvsrplusplusrestore" aria-hidden="true">#</a> BasicVSRPlusPlusRestore</h3><p>A real-world super-resolution algorithm that depends on the length of the super-resolution sequence for effect.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>This algorithm is only available in the beta version of the public test.</p></div><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>This series of algorithms consume a lot of video memory, it is recommended to use a graphics card with more than 6G.</p></div><ul><li>basicvsrpp_ntire_t3_decompress_max_4x 4x magnification decompression model t3 (recommended)</li><li>basicvsrpp_ntire_t3_decompress_max_4x_trt 4x magnification decompression model t3 (TensorRT acceleration) (difficult to compile, not recommended)</li></ul><h3 id="anime4k" tabindex="-1"><a class="header-anchor" href="#anime4k" aria-hidden="true">#</a> Anime4K</h3><p>A super-fast real-time anime super-resolution algorithm, relatively conservative</p><p>There are 6 preset scripts in total.</p><ul><li>Anime4K_Upscale_x2 A/B/C/D are all 2x magnifications (default is A).</li><li>Anime4K_Upscale_x3 is 3x magnification, and the x4 model is similar.</li></ul><h4 id="custom-anime4k-models" tabindex="-1"><a class="header-anchor" href="#custom-anime4k-models" aria-hidden="true">#</a> Custom Anime4K models</h4><ul><li>In the installation folder <code>models\\sr\\Anime4K\\models</code>, you can see the <code>.json</code> model configuration file.</li><li>Take <code>Anime4K_Upscale_x2_A.json</code> as an example.</li></ul><div class="language-json line-numbers-mode" data-ext="json"><pre class="language-json"><code><span class="token punctuation">{</span>
  <span class="token property">&quot;shaders&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
      <span class="token property">&quot;path&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Restore/Anime4K_Clamp_Highlights.glsl&quot;</span><span class="token punctuation">,</span> <span class="token property">&quot;args&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span>
      <span class="token property">&quot;path&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Restore/Anime4K_Restore_CNN_VL.glsl&quot;</span><span class="token punctuation">,</span> <span class="token property">&quot;args&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span>
      <span class="token property">&quot;path&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Upscale/Anime4K_Upscale_CNN_x2_VL.glsl&quot;</span><span class="token punctuation">,</span> <span class="token property">&quot;args&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">&quot;upscale&quot;</span><span class="token punctuation">]</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li><p>Among them, <code>Anime4K_Clamp_Highlights.glsl</code> and <code>Anime4K_Restore_CNN_VL.glsl</code> are 1x restoration algorithms, corresponding to <code>models\\sr\\Anime4K\\Restore\\Anime4K_Clamp_Highlights.glsl</code>. The <code>args</code> parameter of this model needs to be left empty.</p></li><li><p><code>Anime4K_Upscale_CNN_x2_VL.glsl</code> is a 2x magnification algorithm, corresponding to <code>models\\sr\\Anime4K\\Upscale\\Anime4K_Upscale_CNN_x2_VL.glsl</code>. The <code>args</code> parameter of this model needs to be filled in with <code>upscale</code>.</p></li><li><p>Similar to the <code>Anime4K_AutoDownscalePre_x2.glsl</code> model, the <code>args</code> parameter needs to be filled in with <code>downscale</code>.</p></li><li><p>The order of the list is the actual calling order of the filters, and you can observe the model folder to freely combine, edit or create a new <code>.json</code> file to take effect.</p></li></ul><h3 id="waifu2x" tabindex="-1"><a class="header-anchor" href="#waifu2x" aria-hidden="true">#</a> waifu2x</h3><p>A classic conservative super-resolution algorithm</p><ul><li>The cunet model is used for anime super-resolution.</li><li>The photo model is used for real-world shooting.</li><li>anime is used for anime super-resolution.</li></ul><h3 id="compact" tabindex="-1"><a class="header-anchor" href="#compact" aria-hidden="true">#</a> Compact</h3><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>This algorithm is only available in the beta version of the public test of the professional DLC, and you need to manually go to the Steam settings - beta version to select it.</p></div><p>A super-resolution model structure, some models such as <strong>AnimeJanai</strong> are trained based on this structure.</p><h4 id="animejanai" tabindex="-1"><a class="header-anchor" href="#animejanai" aria-hidden="true">#</a> AnimeJanai</h4><p><strong>Applicable to both 3D anime, more suitable for anime</strong></p><ul><li>A weakened version of RealCUGAN, with poor depth-of-field recognition (easy to sharpen the background), less computing power and faster speed.</li><li>Speed: UltraSuper &gt; Super &gt; Compact model.</li></ul><h3 id="span" tabindex="-1"><a class="header-anchor" href="#span" aria-hidden="true">#</a> SPAN</h3><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>This algorithm is only available in the beta version of the public test of the professional DLC, and you need to manually go to the Steam settings - beta version to select it.</p></div><p>A super-resolution model structure, some model series such as <strong>Nomos</strong> are trained based on this structure.</p><h3 id="tensorrt" tabindex="-1"><a class="header-anchor" href="#tensorrt" aria-hidden="true">#</a> TensorRT</h3><p>Dedicated acceleration for the NVIDIA GPU of some of the above super-resolution algorithms</p><ul><li>All models of cugan can be accelerated.</li><li>real-animevideov3 is a model specifically prepared for anime video super-resolution in RealESR.</li><li>RealESRGANv2-animevideo-xsx2 2x anime video super-resolution magnification model.</li><li>RealESRGANv2-animevideo-xsx4 4x anime video super-resolution magnification model.</li></ul><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>Since pre-compilation is required for processing using TRT, do not enable more than 1 thread when using TRT encoding for the first time.</p><p>If an error occurs when using it for the first time, please try five or six times.</p><p>If the error still occurs, please contact the developer.</p><p>In theory, the effect is the same as the non-TRT version, but there are differences in individual scenarios.</p></div><h2 id="visual-comparison-demonstration-of-super-resolution-models" tabindex="-1"><a class="header-anchor" href="#visual-comparison-demonstration-of-super-resolution-models" aria-hidden="true">#</a> Visual Comparison Demonstration of Super-Resolution Models</h2>`,43),Z=e("h2",{id:"add-super-resolution-models-on-openmodeldb-by-yourself",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#add-super-resolution-models-on-openmodeldb-by-yourself","aria-hidden":"true"},"#"),t(" Add Super-Resolution Models on OpenModelDB by Yourself")],-1),$=e("p",null,"SVFI supports adding super-resolution model weights that meet the requirements by oneself.",-1),ee={href:"https://openmodeldb.info/",target:"_blank",rel:"noopener noreferrer"},te=e("div",{align:"center"},[e("img",{src:h,width:"600"})],-1),ie=n('<h3 id="example-adding-compact-or-compact-model" tabindex="-1"><a class="header-anchor" href="#example-adding-compact-or-compact-model" aria-hidden="true">#</a> Example: Adding Compact or Compact Model</h3><ul><li>Search for Aniscale, and you can see the model to be tested, AniScale-2-Compact</li></ul><div align="center"><img src="'+u+'" width="600"></div><ul><li>Click to enter the first generation of Aniscale.</li><li>Pay attention to the model information Size on the right side, <code>64nf</code> represents the number of features (&quot;model channel number&quot;), and <code>16nc</code> represents the number of convolutions (&quot;model depth&quot;).</li></ul><div align="center"><img src="'+m+'" width="600"></div><ul><li><p>The strategy for SVFI to load Compact models is as follows:</p><ul><li>If the model name contains <code>super ultra</code> (from animejanai), <code>nf=24, nc=8</code>;</li><li>If the model name contains <code>ultra</code> (from animejanai), <code>nf=64, nc=8</code>;</li><li>Default <code>nf=64, nc=16</code>.</li></ul></li><li><p>Looking back at Aniscale-2-Compact, it is found that there is no model information description on the web page, so it is considered that it uses the default model structure configuration, <code>nf=64, nc=16</code>.</p></li><li><p>Just download the pth model directly to <code>SVFI\\models\\sr\\Compact\\models</code> and it can be used. If there is no such folder, please create it manually.</p></li><li><p>The same is true for importing the SPAN model.</p></li></ul><div align="center"><img src="'+f+'" width="600"></div><p>SVFI can currently only load models with <code>nf=48</code>, and other models are not supported for the time being. Other modified models are also not supported.</p><h3 id="example-adding-tensorrt-model" tabindex="-1"><a class="header-anchor" href="#example-adding-tensorrt-model" aria-hidden="true">#</a> Example: Adding TensorRT Model</h3>',9),ne={href:"https://github.com/the-database/mpv-upscale-2x_animejanai",target:"_blank",rel:"noopener noreferrer"},ae=n('<p>The onnx requirements of the super-resolution model supported by SVFI are as follows:</p><ul><li>There is only one input, and the dimension is <code>[dynamic, 3, dynamic, dynamic]</code>.</li><li>There is only one output, and the dimension is <code>[dynamic, 3, dynamic, dynamic]</code>.</li><li>The input node name is <code>input</code>, and the output node name is <code>output</code>.</li></ul><p>Put it in <code>SVFI\\models\\sr\\TensorRT\\models</code>.</p><div class="hint-container tip"><p class="hint-container-title">Model Compilation Instructions</p><ul><li>After the model is compiled, a <code>.engine</code> file will be generated. For example, <code>realesrgan_2x.onnx.540x960_workspace128_fp16_io32_device0_8601.engine</code> indicates that the input size (cutting size) of the model is 540x960.</li><li>Different cutting sizes will lead to completely different super-resolution speeds, so the cutting block size should be carefully selected, and try not to enable the cutting block.</li></ul></div><h3 id="other-model-rules" tabindex="-1"><a class="header-anchor" href="#other-model-rules" aria-hidden="true">#</a> Other Model Rules</h3><ul><li>Under the default state of esrgan, only models with <code>nf=64, nb=23</code> are supported.</li><li>When the model name contains <code>anime</code>, <code>nb</code> will be recognized as 6.</li></ul><div class="hint-container tip"><p class="hint-container-title">Terminology Explanation</p><ul><li>nf =&gt; number of features,</li><li>nc =&gt; number of convs,</li><li>nb =&gt; number of blocks</li></ul></div><h2 id="introduction-to-some-special-models-placed-in-the-super-resolution-category" tabindex="-1"><a class="header-anchor" href="#introduction-to-some-special-models-placed-in-the-super-resolution-category" aria-hidden="true">#</a> Introduction to Some Special Models Placed in the Super-Resolution Category</h2><h3 id="inpaint-watermark-removal-model" tabindex="-1"><a class="header-anchor" href="#inpaint-watermark-removal-model" aria-hidden="true">#</a> InPaint Watermark Removal Model</h3><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>This algorithm is only available in the beta version of the professional DLC, and you need to manually go to the Steam settings - beta version to select it.</p></div><ul><li>inpaint_sttn_1x: Currently, this model only supports one-time restoration and has no super-resolution function. It needs to be used with the mask function:</li></ul><p>The activation process is as follows:</p><ol><li>Enable the super-resolution function and select the correct model</li></ol><div align="center"><img src="'+I+'" width="600"></div><ol start="2"><li>Enable the player function</li></ol><div align="center"><img src="'+A+'" width="600"></div><ol start="3"><li>Enable the mask function</li></ol><div align="center"><img src="'+R+'" width="600"></div><ol start="4"><li>Draw the mask and save it</li></ol><div align="center"><img src="'+q+'" width="600"></div><p>This model will automatically identify and remove static watermarks in each mask area. Please make sure there is enough <strong>dynamic change content</strong> in the mask area, otherwise it cannot be automatically identified.</p><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>This model has poor performance in identifying and removing watermarks on <strong>solid background/static content</strong>.</p></div><ol start="5"><li>Click Encode to start removing watermarks</li></ol>',23),oe=n('<h2 id="introduction-to-other-super-resolution-options" tabindex="-1"><a class="header-anchor" href="#introduction-to-other-super-resolution-options" aria-hidden="true">#</a> Introduction to Other Super-Resolution Options</h2><h3 id="super-resolution-model-magnification" tabindex="-1"><a class="header-anchor" href="#super-resolution-model-magnification" aria-hidden="true">#</a> Super-Resolution Model Magnification</h3><p>The super-resolution magnification of the currently selected model</p><h3 id="transfer-resolution-ratio" tabindex="-1"><a class="header-anchor" href="#transfer-resolution-ratio" aria-hidden="true">#</a> Transfer Resolution Ratio</h3><p>That is, the pre-scaling function: first scale the original video by the percentage set by the user, and then perform super-resolution</p><blockquote><p>Example: Original video: <strong>1920x1080</strong>, transfer resolution ratio: <strong>50%</strong>, model magnification: <strong>4x</strong></p><p>At this time, the software running process is: <code>1920x1080</code> (input) -&gt; <code>960x540</code> (down-scaled by 50%) -&gt; <code>3840x2160</code> (super-resolution)</p></blockquote><div class="hint-container tip"><p class="hint-container-title">Tips</p><ul><li>For restoration models, the transfer resolution will be forced to be set to 100%.</li></ul></div><div class="hint-container warning"><p class="hint-container-title">Warning</p><ul><li>SVFI will only perform one super-resolution or restoration process on each frame, which means that when the user sets the output resolution to <code>400%</code> but uses the 2x model for super-resolution, SVFI will only super-resolve the original video once to 200% using the super-resolution model, and then stretch it to 400% using bicubic.</li><li>Therefore, using 100% transfer resolution, using a 2x model for 400% super-resolution, and using a 4x model for 200% super-resolution will have different effects.</li><li>When the super-resolution magnification is inconsistent with the model magnification, using the cutting block may cause the output video to be garbled.</li></ul></div><h3 id="tiling-block-mode" tabindex="-1"><a class="header-anchor" href="#tiling-block-mode" aria-hidden="true">#</a> Tiling block mode</h3><p>Dedicated to certain models, the more you cut, the more video memory you save, and the slower the speed</p><ul><li><p>No Tile: Do not use cutting</p></li><li><p>1/2 on Width: Split horizontally in half</p></li><li><p>1/2 on both W and H: Split horizontally and vertically in half</p></li><li><p>1/3 on w &amp; h: Cut horizontally and vertically into three equal parts</p></li><li><p>1/4 on w &amp; h: Cut horizontally and vertically into four equal parts</p></li></ul><h3 id="realcugan-low-video-memory-mode" tabindex="-1"><a class="header-anchor" href="#realcugan-low-video-memory-mode" aria-hidden="true">#</a> RealCUGAN Low Video Memory Mode</h3><p>Dedicated to realCUGAN, to be used when the video memory of the graphics card is insufficient</p><ul><li><p>None: Do not use the low video memory mode</p></li><li><p>Low VRAM Mode: Enable the low video memory mode, which may affect the picture quality</p></li></ul><h3 id="tiling-size" tabindex="-1"><a class="header-anchor" href="#tiling-size" aria-hidden="true">#</a> Tiling Size</h3><ul><li>There are presets for the size of the video memory, and you can also choose to customize the adjustment</li></ul><div class="hint-container tip"><p class="hint-container-title">Suggested operation when encountering video memory shortage</p><ul><li>For graphics cards with less than 6G of video memory, if the video memory is insufficient, directly enable the cutting block, and keep other options default.</li><li>For more than 6G, try not to enable the cutting block, and enable the cutting mode. If it is still out of video memory when the maximum (1/4) is turned on, turn off this setting and directly enable the cutting block, and try the options from large (512) to small in turn.</li><li>For 4G or less video memory, please enable the low video memory mode and directly enable the cutting block.</li></ul></div><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>It is not recommended to enable when using realCUGAN</p></div><h3 id="super-resolution-strength" tabindex="-1"><a class="header-anchor" href="#super-resolution-strength" aria-hidden="true">#</a> Super-Resolution Strength</h3><p>Only used for the RealCUGAN super-resolution model series</p><p>For non-TensorRT models: the smaller the value, the clearer and sharper the image, and the larger the value, the more conservative and stable (recommended value range 0.5-1.2)</p><p>For TensorRT models, the opposite is true: the smaller the value, the more blurred the image, and the upper limit is 1.</p><h3 id="super-resolution-threads" tabindex="-1"><a class="header-anchor" href="#super-resolution-threads" aria-hidden="true">#</a> Super-Resolution Threads</h3><p>When there are multiple graphics cards or the graphics card occupancy is not fully utilized, you can try to increase this value (by 1 at a time)</p><h3 id="super-resolution-sequence-length" tabindex="-1"><a class="header-anchor" href="#super-resolution-sequence-length" aria-hidden="true">#</a> Super-Resolution Sequence Length</h3><p>Only valid when algorithms such as BasicVSR series and InPaint that require multi-frame input are selected</p><ul><li>The larger the super-resolution sequence length, the more frames are input in a single super-resolution, and the texture is more stable, but at the same time, the video memory usage will increase.</li><li>It is recommended to keep this value above 10. If the video memory is insufficient, it is recommended to reduce the picture resolution and ensure that the value is above 5.</li><li>For the watermark removal (InPaint) model, this value is generally recommended to be above 30 to obtain a better watermark removal effect.</li></ul><h3 id="super-resolution-using-half-precision" tabindex="-1"><a class="header-anchor" href="#super-resolution-using-half-precision" aria-hidden="true">#</a> Super-Resolution Using Half-Precision</h3><ul><li>It is recommended to enable, which can greatly reduce the video memory usage and have little impact on the picture quality.</li></ul><div class="hint-container caution"><p class="hint-container-title">Caution</p><p>When using NVIDIA 10xx series Pascal architecture graphics cards, enabling this option will slow down the super-resolution speed and may cause the output to be black. It is recommended to turn off this option.</p></div><h3 id="tta" tabindex="-1"><a class="header-anchor" href="#tta" aria-hidden="true">#</a> TTA</h3><p>Only supported by ncnnCUGAN, in exchange for a small improvement in image quality at the cost of a large amount of time consumption</p><h3 id="using-ai-enhancement-algorithms" tabindex="-1"><a class="header-anchor" href="#using-ai-enhancement-algorithms" aria-hidden="true">#</a> Using AI Enhancement Algorithms</h3><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>This feature is only available in the beta version of the public test</p></div><p><strong>FMNet - SDR2HDR10</strong>: Use AI algorithm to convert SDR video to HDR10<br><strong>DeepDeband</strong>: Use AI algorithm to remove color bands (this may cause the picture color to turn pink)</p><h2 id="output-settings-encode-parameter-quality" tabindex="-1"><a class="header-anchor" href="#output-settings-encode-parameter-quality" aria-hidden="true">#</a> Output Settings (Encode Parameter Quality)</h2><div align="center"><img src="'+F+'" width="600"></div><h3 id="rendering-quality-crf" tabindex="-1"><a class="header-anchor" href="#rendering-quality-crf" aria-hidden="true">#</a> Rendering Quality CRF</h3><p>Used to adjust the quality loss when the video is exported, which is <strong>positively correlated with the output bitrate</strong>.</p><p>Using different compression codecs and compression presets will have an impact on CRF.</p><p><strong>The CRF numerical parameter is generally 16</strong>, which is lossless to the naked eye;</p><p>For H.265 encoding, the bitrate will be significantly reduced. <strong>Please use the visual quality of the picture to determine whether the CRF numerical value is reasonable.</strong></p><p>If it is used as a <strong>collection, the CRF numerical parameter can be set to 12</strong>.</p><p><strong>The smaller the CRF value, the less the loss of the picture after the operation, and the larger the volume (bitrate) of the exported finished video.</strong></p><p><strong>Note: For the same value, the output quality of different codecs is different</strong></p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>When adjusting the output video bitrate, if you are not familiar with CRF, please use the default parameter 16 or learn relevant knowledge through Baidu.</p></div><h3 id="target-bitrate" tabindex="-1"><a class="header-anchor" href="#target-bitrate" aria-hidden="true">#</a> Target Bitrate</h3><p>As an alternative option to render quality CRF, it is basically the same as the settings standards of Premier Pro, After Effects, and DaVinci Resolve</p><h3 id="codec" tabindex="-1"><a class="header-anchor" href="#codec" aria-hidden="true">#</a> Codec</h3><ul><li><strong>AUTO</strong><br> Automatically determine the codec option based on the slider below the software</li><li><strong>CPU</strong><br> Select this option for compression, <strong>the quality is the highest, but the CPU usage rate is also the highest</strong>. The <strong>performance of the CPU</strong> determines whether the frame interpolation or super-resolution process will be blocked (resulting in a decrease in the graphics card usage) and the <strong>length of time</strong> it takes to complete the final operation.</li><li><strong>NVENC</strong><br> This option is only for <strong>NVIDIA graphics cards that support the NVENC function</strong>. If your graphics card <strong>does not support the NVENC function, please do not select this option</strong>.<br> Please refer to the NVIDIA NVENC Gen.pdf in the installation directory to check whether your graphics card supports NVENC</li><li><strong>VCE</strong><br> This option is only for <strong>AMD graphics cards that support the VCE function</strong>. If your graphics card <strong>does not support the VCE function, please do not select this option</strong>.</li><li><strong>QSV</strong><br> This option is only for users with <strong>Intel integrated graphics</strong> (such as Intel UHD 630, IrisPro 580). Non-such users should not select this option.</li></ul>',50),re={class:"hint-container tip"},se=e("p",{class:"hint-container-title"},"Tips",-1),le={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},de=n('<ul><li><strong>NVENCC</strong> is an optimized version of <strong>NVENC</strong>, with faster processing speed and better work quality.</li><li><strong>QSVENCC</strong> is an optimized version of <strong>QSV</strong>, with higher efficiency in completing tasks.</li><li><strong>VCENCC</strong> is an optimized version of <strong>VCE</strong>, with higher efficiency in completing tasks.</li></ul><div class="hint-container tip"><p class="hint-container-title">Manually specify the GPU used by the hardware encoder</p><p>In the custom compression command line option of the advanced settings,</p><ul><li>When using the encc encoder, fill in <code>-d||&lt;gpu&gt;</code> to control the used encoding GPU, such as <code>-d||0</code></li><li>When using the ffmpeg nvenc encoder, fill in <code>-gpu||&lt;gpu&gt;</code> to control the used encoding GPU</li><li>When using the ffmpeg vce, qsv encoder, fill in <code>-init_hw_device||qsv=intel,child_device=&lt;gpu&gt;</code> to control the used encoding GPU</li></ul></div><p>Sensible comparison:</p><table><thead><tr><th>Codec</th><th>Use Hardware</th><th>Speed</th><th>Quality</th><th>File Size</th><th>Selection Suggestions</th></tr></thead><tbody><tr><td>CPU</td><td>CPU</td><td>Medium</td><td>High</td><td>Medium</td><td>Users who pursue image quality and encoding stability, and AMD GPU users and AU users</td></tr><tr><td>NVENC</td><td>NVIDIA GPU</td><td>Fast</td><td>Medium</td><td>Large</td><td>Users who pursue both speed and quality, and are not sensitive to size</td></tr><tr><td>QSV</td><td>Intel Integrated Graphics</td><td>Fast</td><td>Medium</td><td>Large</td><td>Users who pursue both speed and quality, and are not sensitive to size</td></tr></tbody></table><h3 id="select-the-compression-codec" tabindex="-1"><a class="header-anchor" href="#select-the-compression-codec" aria-hidden="true">#</a> Select the compression codec</h3><p>For the selection of this function, you need to have certain <strong>video compression knowledge</strong>.</p><div class="hint-container warning"><p class="hint-container-title">If you are not familiar with compression, please keep the following rules in mind:</p><ul><li>HDR output must select <strong>H.265 10bit</strong> encoding</li><li>For resolutions above 2K, <strong>H.265</strong> encoding must be selected: especially 4K, 8K resolutions</li><li>If there are problems with both H.264 and H.265 encoding, use <strong>ProRes</strong> encoding. This encoding output is closest to the lossless to the naked eye, and the bitrate is extremely large. It is an intermediate encoding format used for editing work.</li><li>It is recommended to use H.265 fast encoding or ProRes encoding</li><li>When a <code>Broken Pipe</code> error occurs, please directly use H265 encoding. Please note that the above encoding has the highest resolution and frame rate limitations,</li><li>Please do not deliberately pursue too high a resolution and frame rate at the same time: such as 8K 120fps</li></ul></div>',7),he={class:"hint-container tip"},ce=e("p",{class:"hint-container-title"},"Tips",-1),pe=e("li",null,[t("CPU encoding is software encoding, and software encoding "),e("strong",null,"generally has slow speed, small files, and good quality"),t(".")],-1),ue=e("li",null,"NVENC, QSV, and VCE are hardware encodings, where NVENC uses nVidia graphics cards, QSV uses Intel integrated graphics, and VCE uses AMD graphics cards. The characteristics of hardware encodings are fast speed, large files, and in the case of low bitrate and small files, the quality is worse than that of the CPU.",-1),me={href:"https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new",target:"_blank",rel:"noopener noreferrer"},fe=e("li",null,[t("Hard encoding will put a certain load on the graphics card. If the "),e("strong",null,"Broken Pipe"),t(" error occurs when using NVENC, please "),e("strong",null,"reduce the NVIDIA GPU hard encoding preset or switch to the core display encoding QSV"),t(".")],-1),ge=e("li",null,[t("If there is still the same error, "),e("strong",null,"use the CPU"),t(".")],-1),be=n('<div class="hint-container tip"><p class="hint-container-title">Other general suggestions</p><ul><li>If the output is only for personal viewing and the requirements for compression quality are low, please try to use hardware encoding (NVENC, VCE, QSV, etc.) to avoid CPU compression bottlenecks. CPU bottlenecks will cause a decrease in the graphics card usage rate, which in turn will cause a decrease in the task speed</li></ul></div><h3 id="select-the-compression-preset" tabindex="-1"><a class="header-anchor" href="#select-the-compression-preset" aria-hidden="true">#</a> Select the compression preset</h3><ul><li><p>CPU: The English meaning is <strong>the faster the speed, the lower the quality, and vice versa</strong>.</p></li><li><p>NVENC (edicated to NVIDIA GPU): It is recommended to select p7 without thinking</p></li><li><p>QSV (dedicated to Intel graphics card): Select slow directly</p></li><li><p>VCE (dedicated to AMD GPU): Select quality directly</p></li><li><p>NVENCC (dedicated to NVIDIA GPU): Select quality directly</p></li><li><p>QSVENCC (dedicated to Intel graphics card): Select best directly</p></li><li><p>VCENCC (dedicated to AMD GPU): Select slow directly</p></li></ul><h3 id="use-zero-delay-decoding-and-encoding" tabindex="-1"><a class="header-anchor" href="#use-zero-delay-decoding-and-encoding" aria-hidden="true">#</a> Use zero-delay decoding and encoding</h3><p>It is only valid when H264 or H265 is selected at the compression codec.</p><p>Using this feature can reduce the video decoding pressure, and is suitable for scenarios that require fast decoding and low latency, such as:</p><ul><li>When uploading video works to platforms such as BiliBili and Youtube, to avoid jitter transcoding</li><li>When playing ultra-high-definition and ultra-high-frame-rate content on VR headsets</li><li>When the player decodes the screen is distorted</li></ul><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>This feature does not work when the input is HDR</p></div><h3 id="nvidia-gpu-hard-encoding-preset" tabindex="-1"><a class="header-anchor" href="#nvidia-gpu-hard-encoding-preset" aria-hidden="true">#</a> NVIDIA GPU hard encoding preset</h3><p>When choosing the NVENC encoder, the NVIDIA GPU hard encoding preset can reduce the export video size without changing the picture quality. You need to query which generation of NVENC compression chip your NVIDIA GPU is. If it exceeds 7th, directly select 7th+.</p><h3 id="default-compression-scheme" tabindex="-1"><a class="header-anchor" href="#default-compression-scheme" aria-hidden="true">#</a> Default compression scheme</h3><p>Using the traditional compression scheme, the compatibility is strong, and the export video size may increase.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>Enabling this feature can solve most broken pipe problems.</p></div><h3 id="secondary-compression-audio-quality" tabindex="-1"><a class="header-anchor" href="#secondary-compression-audio-quality" aria-hidden="true">#</a> Secondary compression audio quality</h3><ul><li>Re-encode the audio, generally used on videos uploaded to the platform</li><li>Compress all audio tracks in the video to <strong>640kbps aac format</strong>.</li></ul><h3 id="hdr-strict-mode" tabindex="-1"><a class="header-anchor" href="#hdr-strict-mode" aria-hidden="true">#</a> HDR Strict Mode</h3><p>Process HDR content with strict presets, enabled by default</p><h3 id="compatible-with-dv-hdr10" tabindex="-1"><a class="header-anchor" href="#compatible-with-dv-hdr10" aria-hidden="true">#</a> Compatible with DV HDR10</h3><p>Enable HDR10 compatibility when outputting Dolby Vision, enabled by default</p><h3 id="one-click-hdr-convert-sdr-video-to-hdr10" tabindex="-1"><a class="header-anchor" href="#one-click-hdr-convert-sdr-video-to-hdr10" aria-hidden="true">#</a> One-click HDR: Convert SDR video to HDR10+</h3><p>Four one-click HDR modes need to be tried by yourself</p><h2 id="decoding-quality-control" tabindex="-1"><a class="header-anchor" href="#decoding-quality-control" aria-hidden="true">#</a> Decoding Quality Control</h2><h3 id="use-vspipe-for-pre-decoding" tabindex="-1"><a class="header-anchor" href="#use-vspipe-for-pre-decoding" aria-hidden="true">#</a> Use vspipe for pre-decoding</h3>',23),ve={class:"hint-container tip"},ye=e("p",{class:"hint-container-title"},"Tips",-1),we={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},_e=e("p",null,"Using vspipe as pre-decoding, this function is a prerequisite for many specific functions (such as deblocking, quick noise addition, QTGMC deinterlacing),",-1),xe=e("p",null,"If you find that it cannot decode the input or the task reports an error, please turn off this option.",-1),ke=e("h3",{id:"full-vspipe-workflow",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#full-vspipe-workflow","aria-hidden":"true"},"#"),t(" Full VSPipe Workflow")],-1),Se={class:"hint-container tip"},Te=e("p",{class:"hint-container-title"},"Tips",-1),Ce={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},Ie=n('<p>The entire process is processed using vspipe to reduce unnecessary calculations. It is the mode that can achieve the fastest speed under the same settings of the current SVFI.</p><p>Only supports TensorRT-accelerated super-resolution and some frame interpolation models.</p><p>If this setting is enabled for frame interpolation, the <code>spatio-temporal linearization</code> smoothness optimization will be forced to be enabled.</p><h3 id="hardware-decoding" tabindex="-1"><a class="header-anchor" href="#hardware-decoding" aria-hidden="true">#</a> Hardware Decoding</h3><p>It can reduce the decoding pressure of large-resolution videos, but may <strong>reduce the picture quality</strong> to a certain extent, and cause the frame interpolation module to <strong>run out of video memory</strong> when the video memory is tight.</p><h3 id="fast-frame-splitting" tabindex="-1"><a class="header-anchor" href="#fast-frame-splitting" aria-hidden="true">#</a> Fast Frame Splitting</h3><p>Fast frame splitting operation can <strong>reduce decoding pressure</strong>, but may <strong>cause color deviation in the picture</strong>.</p><h3 id="high-precision-optimization-workflow" tabindex="-1"><a class="header-anchor" href="#high-precision-optimization-workflow" aria-hidden="true">#</a> High-Precision Optimization Workflow</h3>',8),Ae={class:"hint-container tip"},Re=e("p",{class:"hint-container-title"},"Tips",-1),qe={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},Fe=n('<ul><li>If the CPU performance is excessive, it is recommended to enable this feature, which can <strong>solve most color deviation problems</strong> and can solve the color cast problem caused by HDR video compression to the greatest extent. This feature will <strong>increase the CPU burden</strong> and even affect the frame interpolation speed.</li><li>Enabling this feature for super-resolution work will <strong>disable half-precision</strong> (requires more video memory). Please <strong>choose according to your needs</strong>.</li></ul><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>It is recommended to enable this option when inputting HDR videos.</p></div><h3 id="enable-deinterlacing" tabindex="-1"><a class="header-anchor" href="#enable-deinterlacing" aria-hidden="true">#</a> Enable Deinterlacing</h3>',3),Pe={class:"hint-container tip"},Ue=e("p",{class:"hint-container-title"},"Tips",-1),De={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},Ve=e("ul",null,[e("li",null,[e("p",null,[t("Use "),e("strong",null,"ffmpeg"),t(" to perform deinterlacing processing on the input "),e("strong",null,"interlaced video"),t(".")])]),e("li",null,[e("p",null,"When using vspipe for pre-decoding, use QTGMC deinterlacing to process the picture.")])],-1),Ne=e("h3",{id:"fast-noise-reduction",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#fast-noise-reduction","aria-hidden":"true"},"#"),t(" Fast Noise Reduction")],-1),Ee={class:"hint-container tip"},Ge=e("p",{class:"hint-container-title"},"Tips",-1),Me={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},ze=n('<p>The &quot;Fast&quot; option in this column, if there is no special need, please keep it closed, otherwise it will <strong>slow down the task processing speed</strong>.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>It is recommended to test this option by controlling the variables yourself to see if it is helpful for improving the picture quality.</p><p>Not compatible with high-precision optimization workflow</p></div><h3 id="quick-noise-addition" tabindex="-1"><a class="header-anchor" href="#quick-noise-addition" aria-hidden="true">#</a> Quick Noise Addition</h3><p>Add noise to the video, often used when super-resolving the video.</p><h3 id="custom-frame-splitting-parameters-professional-option" tabindex="-1"><a class="header-anchor" href="#custom-frame-splitting-parameters-professional-option" aria-hidden="true">#</a> Custom Frame Splitting Parameters (Professional Option)</h3><p>Used to replace the parameters used by ffmpeg or vspipe for decoding, and custom parameters are separated by <code>||</code>.</p><h2 id="custom-encoding-settings" tabindex="-1"><a class="header-anchor" href="#custom-encoding-settings" aria-hidden="true">#</a> Custom Encoding Settings</h2><h3 id="specify-the-number-of-encoding-threads" tabindex="-1"><a class="header-anchor" href="#specify-the-number-of-encoding-threads" aria-hidden="true">#</a> Specify the number of encoding threads</h3><p>When the encoder is CPU, there is a chance to control the CPU usage rate to control the rendering speed.</p><h3 id="custom-compression-parameters" tabindex="-1"><a class="header-anchor" href="#custom-compression-parameters" aria-hidden="true">#</a> Custom Compression Parameters</h3><p>This feature is a professional option (note that the number of input items must be <strong>even</strong>),</p><p>The key values are separated by <code>||</code></p>',12),We=e("p",null,[e("code",null,"-x265-params||ref=4:me=3:subme=4:rd=4:merange=38:rdoq-level=2:rc-lookahead=40:scenecut=40:strong-intra-smoothing=0")],-1),Oe=e("h3",{id:"time-remapping-change-the-speed-of-the-video",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#time-remapping-change-the-speed-of-the-video","aria-hidden":"true"},"#"),t(" Time Remapping: Change the Speed of the Video")],-1),je={class:"hint-container tip"},Le=e("p",{class:"hint-container-title"},"Tips",-1),He={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},Be=e("ul",null,[e("li",null,[e("p",null,'This feature is used to create "slow motion" materials.')]),e("li",null,[e("p",null,[t("For example, if the output frame rate is set to 120 frames and the time remapping is set to 60 frames, the output effect is equivalent to 50% "),e("strong",null,"slow playback of the playback speed"),t(".")])]),e("li",null,[e("p",null,[t("Similar to other situations, you can set the output frame rate by yourself, "),e("strong",null,"support decimals"),t(".")])])],-1),Ke={class:"hint-container warning"},Qe=e("p",{class:"hint-container-title"},"Warning",-1),Ye=e("strong",null,"Video Smoothness Optimization",-1),Je=e("strong",null,"Frame Interpolation Settings",-1),Xe=e("p",null,"Or use software such as Premiere to reduce the frame rate of the original video to remove the repeated frames to avoid jitter after remapping.",-1),Ze=e("p",null,"The frame rate of the original video is generally reduced to 8 or 12 fps",-1),$e=n('<h3 id="start-and-end-looping" tabindex="-1"><a class="header-anchor" href="#start-and-end-looping" aria-hidden="true">#</a> Start and End Looping</h3><p>Put the last frame in the first frame to adapt to some looping videos that are connected end to end.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>Under normal circumstances, the end will miss (output frame rate / input frame rate) frames because there are no new frames that can be filled in, which is normal. But it is not affected in the loop mode, because there are always beginning frames that can be used as frame-filling pairs with the end frames to fill in the remaining frames.</p></div><h2 id="io-control" tabindex="-1"><a class="header-anchor" href="#io-control" aria-hidden="true">#</a> IO Control</h2><div align="center"><img src="'+P+'" width="600"></div><h3 id="manually-specify-buffer-memory-size" tabindex="-1"><a class="header-anchor" href="#manually-specify-buffer-memory-size" aria-hidden="true">#</a> Manually Specify Buffer Memory Size</h3><p>If the running memory is tight (below 16G), it is recommended to <strong>manually specify the size of the buffer memory</strong> to 2-3G to avoid <strong>out of memory</strong> errors.</p><h3 id="single-output-chunk-size" tabindex="-1"><a class="header-anchor" href="#single-output-chunk-size" aria-hidden="true">#</a> Single Output Chunk Size</h3><ul><li>For frame interpolation and compression tasks, every frame rendered for this value will output a small clip without audio for you to <strong>preview the effect</strong>.</li><li>The clips will be generated in the output folder you set, <strong>and merged into one file after the frame interpolation or compression task is completed</strong>.</li></ul><h3 id="retain-the-project-folder-after-the-task-is-completed" tabindex="-1"><a class="header-anchor" href="#retain-the-project-folder-after-the-task-is-completed" aria-hidden="true">#</a> Retain the project folder after the task is completed</h3><p>Do not delete the project folder after the task is completed.</p><h2 id="frame-interpolation-settings" tabindex="-1"><a class="header-anchor" href="#frame-interpolation-settings" aria-hidden="true">#</a> Frame Interpolation Settings</h2><h3 id="safe-frame-rate" tabindex="-1"><a class="header-anchor" href="#safe-frame-rate" aria-hidden="true">#</a> Safe Frame Rate</h3><p>If the video is to be uploaded to the corresponding media platform for online viewing, please enable this option.</p><p>This option will convert the output correctly to the corresponding NTSC format video (such as <code>60000/1001</code>) when the input is an NTSC format video (such as a video with a frame rate of <code>24000/1001</code>), to avoid audio-video asynchrony. If not enabled, audio-video asynchrony may occur (such as an output of <code>59994/1000</code>).</p><p>It is recommended to keep this option enabled</p><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>If this option is not enabled, when the input is a non-standard frame rate (such as 119800/1000), the output mkv may become a variable frame rate video due to mkvmerge.</p><p>Try to use videos with standard input frame rates for processing to avoid audio-video asynchrony</p></div><h3 id="half-precision-mode" tabindex="-1"><a class="header-anchor" href="#half-precision-mode" aria-hidden="true">#</a> Half-Precision Mode</h3><p>It can reduce the video memory usage, and has acceleration effect for NVIDIA graphics cards of 20 series, 30 series, 40 series and above</p><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>May cause a decrease in picture accuracy.<br> For example, when using the gmfss model for frame interpolation, it may cause the output video to have a grainy feel</p></div><h3 id="reverse-optical-flow" tabindex="-1"><a class="header-anchor" href="#reverse-optical-flow" aria-hidden="true">#</a> Reverse Optical Flow</h3><p>This feature can make the picture <strong>smoother</strong> to a certain extent.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>If the <code>cudnn status error</code> occurs when using the GMFSS pg 104 frame interpolation model, please turn off the reverse optical flow.</p><p>Enabling this feature may cause artifacts around moving objects in some models (such as Gmfss pg104). It needs to be selectively enabled or disabled after repeated experiments by yourself. The same applies to other similar functions.</p></div><h3 id="optical-flow-scale" tabindex="-1"><a class="header-anchor" href="#optical-flow-scale" aria-hidden="true">#</a> Optical Flow Scale</h3><p>This is the optical flow resolution scaling factor used by SVFI when performing optical flow calculation using the frame interpolation algorithm. 0.5 means that the input picture is scaled by half and then the optical flow calculation is performed to improve the performance or effect of certain algorithms.</p><ul><li><p>When using the RIFE algorithm, when the original video size is 1080P, the default is 1.0; 4K and above is 0.5; less than 1080P is 1.0</p></li><li><p>When using the GMFSS algorithm, when the original video size is 1080P, the default is 1.0; 4K and above is 0.5; less than 1080P is 1.0</p></li></ul><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>When using the GMFSS algorithm, it is not recommended to fill in a value lower than 1.0 for the option when the original video size is less than or equal to 1080P</p></div><h3 id="interlaced-frame-interpolation" tabindex="-1"><a class="header-anchor" href="#interlaced-frame-interpolation" aria-hidden="true">#</a> Interlaced Frame Interpolation</h3><ul><li><p>Equivalent to a special cut, used to reduce video memory usage, there will be no screen tearing, but the picture will be blurred</p></li><li><p>Choosing this option appropriately can allow a small video memory graphics card to interpolate an ultra-high-resolution (such as 4G to 8K)</p></li></ul><h3 id="video-smoothness-optimization" tabindex="-1"><a class="header-anchor" href="#video-smoothness-optimization" aria-hidden="true">#</a> Video Smoothness Optimization</h3><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>This series of options is only used for anime input or live-action materials with duplicate frames.</p><p>It is not recommended to enable this option for real-shot materials in general.</p></div><table><thead><tr><th>Method</th><th>Application Scenarios</th><th>Speed</th><th>Smoothness</th><th>Number of Jellies</th></tr></thead><tbody><tr><td>Spatio-Temporal Linearization</td><td>Universal</td><td>★★☆</td><td>★☆☆</td><td>☆☆☆</td></tr><tr><td>Fixed Threshold De-Weighting</td><td>Universal</td><td>★★★</td><td>★☆☆</td><td>☆☆☆</td></tr><tr><td>Remove One Frame per Two</td><td>Anime</td><td>★★★</td><td>★★☆</td><td>★☆☆</td></tr><tr><td>Remove One Frame per Two and One Frame per Three</td><td>Anime</td><td>★★★</td><td>★★☆</td><td>☆☆☆</td></tr><tr><td>First-Order Difference De-Weighting</td><td>Anime</td><td>★★☆</td><td>★★☆</td><td>★★☆</td></tr><tr><td>Spatio-Temporal Resampling</td><td>Anime</td><td>★★☆</td><td>★★★</td><td>★★★</td></tr><tr><td>Forward De-Weighting</td><td>Anime</td><td>☆☆☆</td><td>★★★</td><td>☆☆☆</td></tr><tr><td>Cross Reconstruction</td><td>Anime</td><td>★☆☆</td><td>★★★</td><td>☆☆☆</td></tr><tr><td>Smooth Difference</td><td>Anime, One-time Restoration</td><td>/</td><td>/</td><td>/</td></tr></tbody></table><p>Note: <strong>The fewer the number of jellies, the better the video quality; the more stars, the more likely the algorithm will output jellies</strong>.</p><p>Explanation:</p><ul><li><strong>Spatio-Temporal Linearization</strong>: Solves the jitter caused by the asymmetry problem during frame interpolation, and has a certain smooth and stable effect on any video (also known as TruMotion)</li><li><strong>Fixed Threshold De-Weighting</strong>: Used to alleviate the jitter feeling caused by duplicate frames, the general value is 0.2, 0.5, 1.0 or higher for anime</li><li><strong>Remove One Frame per Two</strong>: Recognize and change one frame every two frames in the animation to one frame per one</li><li><strong>Remove One Frame per Two and One Frame per Three</strong>: Recognize and change one frame every three frames and two frames in the animation to one frame per one</li><li><strong>First-Order Difference De-Weighting</strong>: Similar to removing one frame every two and one frame every three, but the de-weighting is more aggressive</li><li><strong>Spatio-Temporal Resampling</strong>: If the input video frame rate is around 24 and there is only one frame every three at most, and there is no higher frame rate picture, the jitter of the anime video material can be completely removed</li><li><strong>Cross Reconstruction</strong>: Similar to spatio-temporal resampling, the overall effect will be better. The input frame rate must be around 24, and the output frame rate can only be an integer multiple of the input frame rate, and is only used for specific models</li><li><strong>Forward De-Weighting</strong>: Completely remove the jitter of the anime video material. If the frame rate of your input video is around 24, the default is 2, which means it can solve the problem of jitter caused by one frame every three or less</li><li><strong>Smooth Difference</strong>: When the video has irregular duplicate frames (not sure how many frames are repeated), or when processing screen recording frame loss videos, this function can be used for a smooth.</li></ul><div class="hint-container warning"><p class="hint-container-title">Warning</p><p><strong>Smooth Difference</strong> does not change the frame rate of the input video</p><p>For some videos with long sections of solid color scenes, this option may introduce unnecessary duplicate frames and cause audio-video asynchrony</p></div>',36),et={class:"hint-container tip"},tt=e("p",{class:"hint-container-title"},"Tips",-1),it=e("strong",null,"Forward De-Weighting",-1),nt=e("strong",null,"Cross Reconstruction",-1),at=e("strong",null,"Spatio-Temporal Resampling",-1),ot=e("br",null,null,-1),rt={href:"https://www.zhihu.com/question/27781021",target:"_blank",rel:"noopener noreferrer"},st=e("p",null,"If the output video is still not smooth enough after using the de-weighting optimization, it may be due to the wrong transition detection, and the transition sensitivity threshold needs to be increased",-1),lt=n('<div class="hint-container warning"><p class="hint-container-title">Warning</p><p>Due to the limited ability of AI frame interpolation in anime frame interpolation at this stage, choosing de-weighting will increase the inter-frame motion range, resulting in picture distortion during frame interpolation. Please test and select the best de-weighting mode for each input video by controlling the variables multiple times.</p><p>It is recommended that you choose the de-weighting mode carefully. If you are frame interpolating the entire anime, it is recommended to turn on spatio-temporal linearization or not to turn on removing duplicate frames.</p></div><p><strong>Frame interpolation effect after enabling video smoothness optimization (forward de-weighting)</strong></p><div align="center"><img src="'+g+'" width="600"></div><div align="center"><img src="'+b+'" width="600"></div><h3 id="load-the-graphics-card-1" tabindex="-1"><a class="header-anchor" href="#load-the-graphics-card-1" aria-hidden="true">#</a> Load the Graphics Card</h3><p>Specify which graphics card to use for frame interpolation</p><h2 id="introduction-to-frame-interpolation-algorithms" tabindex="-1"><a class="header-anchor" href="#introduction-to-frame-interpolation-algorithms" aria-hidden="true">#</a> Introduction to Frame Interpolation Algorithms</h2><p>SVFI integrates several frame interpolation algorithms, such as RIFE, GMFSS, UMSS, etc.</p>',8),dt=e("h2",{id:"introduction-to-frame-interpolation-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#introduction-to-frame-interpolation-models","aria-hidden":"true"},"#"),t(" Introduction to Frame Interpolation Models")],-1),ht={class:"hint-container tip"},ct=e("p",{class:"hint-container-title"},"Tips",-1),pt=e("code",null,"ncnn",-1),ut={href:"https://github.com/Tencent/ncnn",target:"_blank",rel:"noopener noreferrer"},mt=n('<ul><li>RIFE: High-speed, popular new era frame interpolation algorithm (the following is the model introduction)</li></ul><blockquote><p>2.3: Classic, popular model, fast speed, good effect.</p><p>4.6: The speed is more than twice as fast as 2.3, the effect is better, and it is recommended to use.</p><p>4.8: Anime material optimization model, the effect of interpolating anime is better, and the speed is the same as 4.6</p><p>4.9: Anime and live-action material optimization model, the effect of interpolating live-action is better, and the speed is the same</p><p>rpr_v7_2.3_ultra: Combined model, more adaptable to complex scenes.</p><p>rpr_v7_2.3_ultra#2: Combined model, more adaptable to complex scenes.</p></blockquote><ul><li><p>ncnn-rife: RIFE with support for various graphics card versions, good compatibility, fast speed, and slightly worse quality than RIFE.</p></li><li><p>ncnn_dain: Traditional old algorithm, can be used for both anime and live-action, supports any time, very slow speed, and very high smoothness.</p></li><li><p>GMFSS: Slow speed, super high quality (the following is the model introduction) (models with the trt mark are acceleration models)</p></li></ul><blockquote><p>pg104: The fourth-generation gmfss anime model, currently the most powerful anime frame interpolation model</p><p>union_v: The third-generation GMFSS model, with a stable structure and smooth pictures</p><p>basic: The first-generation gmfss model, very slow speed, and the effect may be more stable than up</p></blockquote><h2 id="other-frame-interpolation-options-introduction" tabindex="-1"><a class="header-anchor" href="#other-frame-interpolation-options-introduction" aria-hidden="true">#</a> Other Frame Interpolation Options Introduction</h2><h3 id="tta-mode" tabindex="-1"><a class="header-anchor" href="#tta-mode" aria-hidden="true">#</a> TTA Mode</h3>',6),ft={class:"hint-container tip"},gt=e("p",{class:"hint-container-title"},"Tips",-1),bt={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},vt=n('<blockquote><p>Enabling this feature can <strong>reduce picture jellies, reduce subtitle jitter, and weaken the problem of object disappearance</strong>. Making the picture more <strong>smooth and comfortable</strong></p><p><strong>It takes extra frame interpolation time, and some frame interpolation models do not support this feature</strong>.</p><p>The larger the number behind, the slower, the less jellies, usually just fill in 1 or 2</p><p>Medium to, suitable for RIFE 2.3</p></blockquote><h3 id="bidirectional-optical-flow" tabindex="-1"><a class="header-anchor" href="#bidirectional-optical-flow" aria-hidden="true">#</a> Bidirectional Optical Flow</h3><blockquote><p>The speed is reduced by about half, and the effect of the RIFE 2.x series frame interpolation model may be slightly improved</p><p>The gmfss/umss model enables bidirectional optical flow to accelerate by 5%, the effect will not change, but it will increase the video memory usage</p></blockquote><h3 id="dynamic-optical-flow-scale" tabindex="-1"><a class="header-anchor" href="#dynamic-optical-flow-scale" aria-hidden="true">#</a> Dynamic Optical Flow Scale</h3>',4),yt={class:"hint-container tip"},wt=e("p",{class:"hint-container-title"},"Tips",-1),_t={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},xt=e("blockquote",null,[e("p",null,"During frame interpolation, the optical flow scale is dynamically selected, which can reduce the problem of object disappearance and reduce jellies (only applicable to RIFE 2.3 and RIFE 4.6)")],-1),kt=e("h2",{id:"custom-preset-bar",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#custom-preset-bar","aria-hidden":"true"},"#"),t(" Custom Preset Bar")],-1),St={class:"hint-container tip"},Tt=e("p",{class:"hint-container-title"},"Tips",-1),Ct={href:"https://store.steampowered.com/app/1718750/SVFI_Professional/",target:"_blank",rel:"noopener noreferrer"},It=n('<h3 id="create-a-new-preset-based-on-the-current-settings" tabindex="-1"><a class="header-anchor" href="#create-a-new-preset-based-on-the-current-settings" aria-hidden="true">#</a> Create a New Preset Based on the Current Settings</h3><p>After naming the preset, click to create a new preset</p><h3 id="remove-the-current-preset" tabindex="-1"><a class="header-anchor" href="#remove-the-current-preset" aria-hidden="true">#</a> Remove the Current Preset</h3><p>Delete the currently selected preset</p><h3 id="apply-the-specified-preset" tabindex="-1"><a class="header-anchor" href="#apply-the-specified-preset" aria-hidden="true">#</a> Apply the Specified Preset</h3><p>Load the previously saved preset and automatically load the parameters</p><h2 id="toolbox" tabindex="-1"><a class="header-anchor" href="#toolbox" aria-hidden="true">#</a> Toolbox</h2><h3 id="end-residual-processes" tabindex="-1"><a class="header-anchor" href="#end-residual-processes" aria-hidden="true">#</a> End Residual Processes</h3><p>Will end all tasks, including tasks that open SVFI multiple times.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>If you need to avoid ending the situation of opening SVFI multiple times, you need to manually end all SVFI CLI processes under the current SVFI process in the task manager. When enabling multi-threading, <strong>it is always recommended not to click the</strong> End Task button easily.</p></div><h3 id="convert-video-to-gif-animation" tabindex="-1"><a class="header-anchor" href="#convert-video-to-gif-animation" aria-hidden="true">#</a> Convert Video to GIF Animation</h3><p>Generate high-quality GIF animations</p><p>Usage example:</p><blockquote><p>Input video path: <code>E:\\VIDEO\\video.mp4</code></p><p>Output animation (gif) path: <code>E:\\GIF\\video_gif_output.gif</code></p><p>Output frame rate: 30 fps</p></blockquote><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>The output frame rate generally needs to be less than or equal to the frame rate of the original video, and it is not recommended to be higher than 30</p></div><h3 id="merge-existing-chunks" tabindex="-1"><a class="header-anchor" href="#merge-existing-chunks" aria-hidden="true">#</a> Merge Existing Chunks</h3><p>Merge scattered chunk fragments.</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>If the task fails during the final merge, you can directly select the task and click this button to complete the merge operation after adjusting the settings.</p></div><h3 id="audio-and-video-merging" tabindex="-1"><a class="header-anchor" href="#audio-and-video-merging" aria-hidden="true">#</a> Audio and Video Merging</h3>',19),At=e("code",null,"D:\\01\\myvideo.mp4",-1),Rt=e("code",null,"D:\\01\\myvideo.aac",-1),qt=e("code",null,"D:\\01\\otherVideo.mp4",-1),Ft=e("code",null,"D:\\01\\output.mp4",-1),Pt=e("li",null,[e("p",null,"Secondary compression audio: Compress the audio to aac format, 640kbps")],-1),Ut=e("h3",{id:"export-the-current-settings-to-a-text-file",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#export-the-current-settings-to-a-text-file","aria-hidden":"true"},"#"),t(" Export the Current Settings to a Text File")],-1),Dt=e("br",null,null,-1),Vt=n('<div class="hint-container tip"><p class="hint-container-title">Tips</p><p>If the video output of the software does not meet expectations, such as color cast, poor effect, etc., you can click this button and send the settings file to the developer to locate the problem.</p></div><h3 id="debug-mode" tabindex="-1"><a class="header-anchor" href="#debug-mode" aria-hidden="true">#</a> Debug Mode</h3><p>Output debug information during the task.</p><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>In some cases, this mode will add debug content to the picture and slow down the task processing speed.</p><p>So please turn off this option when processing tasks正式.</p></div><h1 id="left-title-bar-function" tabindex="-1"><a class="header-anchor" href="#left-title-bar-function" aria-hidden="true">#</a> Left Title Bar Function</h1><div align="center"><img src="'+U+'" width="600"></div><h2 id="settings" tabindex="-1"><a class="header-anchor" href="#settings" aria-hidden="true">#</a> Settings</h2><p>Main settings page</p><h2 id="preview" tabindex="-1"><a class="header-anchor" href="#preview" aria-hidden="true">#</a> Preview</h2><p>Output preview page</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>When previewing using the player interface, if the input is an HDR video, it is normal for the preview picture to be gray.</p></div><h2 id="status" tabindex="-1"><a class="header-anchor" href="#status" aria-hidden="true">#</a> Status</h2><p>View the program output information</p><h2 id="user-page" tabindex="-1"><a class="header-anchor" href="#user-page" aria-hidden="true">#</a> User Page</h2><p>View the software achievements and expandable or owned DLCs</p><h2 id="preference-settings" tabindex="-1"><a class="header-anchor" href="#preference-settings" aria-hidden="true">#</a> Preference Settings</h2><h3 id="rest-interval" tabindex="-1"><a class="header-anchor" href="#rest-interval" aria-hidden="true">#</a> Rest Interval</h3><p>Let the device rest for every X hours (temporarily pause the task)</p><h3 id="cache-folder" tabindex="-1"><a class="header-anchor" href="#cache-folder" aria-hidden="true">#</a> Cache Folder</h3><p>Specify the task folder to another location. The final output video will still be in the target folder</p><h3 id="after-the-task-runs" tabindex="-1"><a class="header-anchor" href="#after-the-task-runs" aria-hidden="true">#</a> After the Task Runs</h3><p>You can choose some automatic operations after the frame interpolation is completed</p><h3 id="force-exit" tabindex="-1"><a class="header-anchor" href="#force-exit" aria-hidden="true">#</a> Force Exit</h3><p>Default is enabled, the software forcibly ends the software process when an error occurs, avoiding residual processes</p><h3 id="true-coding-mode" tabindex="-1"><a class="header-anchor" href="#true-coding-mode" aria-hidden="true">#</a> True Coding Mode</h3><p>When encoding, do not perform operations such as duplicate frame de-weighting</p><h3 id="enable-preview" tabindex="-1"><a class="header-anchor" href="#enable-preview" aria-hidden="true">#</a> Enable Preview</h3><p>Enable the preview window when frame interpolation</p><h3 id="auto-error-correction" tabindex="-1"><a class="header-anchor" href="#auto-error-correction" aria-hidden="true">#</a> Auto Error Correction</h3><p>Automatically modify settings to prevent task errors</p><div class="hint-container tip"><p class="hint-container-title">Tips</p><p>Turning off auto error correction can improve the task initialization speed. It is recommended to turn off this option when processing queue tasks with stable settings</p></div><h3 id="clear-the-task-list-after-completion" tabindex="-1"><a class="header-anchor" href="#clear-the-task-list-after-completion" aria-hidden="true">#</a> Clear the Task List After Completion</h3><p>Clear the input queue after all tasks in the list are completed</p><h3 id="mute-mode" tabindex="-1"><a class="header-anchor" href="#mute-mode" aria-hidden="true">#</a> Mute Mode</h3><p>Do not pop up windows and notifications</p><h3 id="background-image" tabindex="-1"><a class="header-anchor" href="#background-image" aria-hidden="true">#</a> Background Image</h3><p>You can select pictures to enable custom backgrounds</p><h3 id="background-blur" tabindex="-1"><a class="header-anchor" href="#background-blur" aria-hidden="true">#</a> Background Blur</h3><p>The larger the value, the more blurred the background</p><h3 id="background-transparency" tabindex="-1"><a class="header-anchor" href="#background-transparency" aria-hidden="true">#</a> Background Transparency</h3><p>The larger the value, the higher the background brightness</p><h3 id="apply-theme" tabindex="-1"><a class="header-anchor" href="#apply-theme" aria-hidden="true">#</a> Apply Theme</h3><p>Change the theme of the application</p><h3 id="theme-color" tabindex="-1"><a class="header-anchor" href="#theme-color" aria-hidden="true">#</a> Theme Color</h3><p>Change the theme color of the application</p><h3 id="language" tabindex="-1"><a class="header-anchor" href="#language" aria-hidden="true">#</a> Language</h3><p>Set the preferred language for the user interface</p><h3 id="use-only-cpu" tabindex="-1"><a class="header-anchor" href="#use-only-cpu" aria-hidden="true">#</a> Use Only CPU</h3><p>Perform AI reasoning only using the CPU. Only applicable to devices without graphics cards.</p><h3 id="use-all-gpus" tabindex="-1"><a class="header-anchor" href="#use-all-gpus" aria-hidden="true">#</a> Use All GPUs</h3><p>Use all available GPUs for AI reasoning acceleration.</p><div class="hint-container warning"><p class="hint-container-title">Warning</p><p>If the device has only one graphics card, please be sure to turn off this option.</p></div><h3 id="tensorrt-int8-quantization-function" tabindex="-1"><a class="header-anchor" href="#tensorrt-int8-quantization-function" aria-hidden="true">#</a> TensorRT INT8 Quantization Function</h3><p>Accelerate the running speed of TensorRT models, but it will take more time to compile TensorRT, <strong>and may cause the model effect to decline</strong>, please use this feature with caution.</p>',54),Nt=n('<h3 id="help" tabindex="-1"><a class="header-anchor" href="#help" aria-hidden="true">#</a> Help</h3><p>Learn about new features and useful tips of SVFI (shortcut operations, shortcut keys, etc.)</p><h3 id="provide-feedback" tabindex="-1"><a class="header-anchor" href="#provide-feedback" aria-hidden="true">#</a> Provide Feedback</h3><p>Provide feedback to help us improve SVFI</p><h3 id="about" tabindex="-1"><a class="header-anchor" href="#about" aria-hidden="true">#</a> About</h3><p>Software copyright and logs</p>',6);function Et(Gt,Mt){const a=l("ExternalLinkIcon"),o=l("Badge"),r=l("imgSlider"),s=l("RouterLink");return y(),w("div",null,[V,e("p",null,[t("The Json path option fills in the path of the "),N,t(" exported by the Transition Chooser for the video, see "),e("a",E,[t("Usage Tutorial"),i(a)]),t(".")]),G,e("blockquote",null,[e("p",null,[t("Example: Video "),i(o,{text:"Note"}),t(" resolution 3840x2160, actual picture resolution 3840x1620, then the "),M,t(" here is filled in as "),z,t(".")])]),i(o,{text:"Note",vertical:"middle"}),t(": If AI super-resolution is used, the video here refers to the final output video"),W,e("div",O,[j,e("p",null,[t("This feature requires the purchase of the "),e("a",L,[t("Professional DLC"),i(a)]),t(".")])]),H,e("ul",null,[B,e("li",null,[t("The pro model is an enhanced version, see "),e("a",K,[t("official introduction"),i(a)]),t(" for details.")]),Q,Y,J]),X,i(r,{items:[{first:"/Statics/UserGuide/SrCompare/t3_in.png",second:"/Statics/UserGuide/SrCompare/t3_out.png",name:"T3",desc:"basicvsrpp-ntire-t3-decompress-max-4x, excellent restoration on highly compressed with slow speed"}]},null,8,["items"]),i(r,{items:[{first:"/Statics/UserGuide/SrCompare/aniscale_in.png",second:"/Statics/UserGuide/SrCompare/aniscale_out.png",name:"Aniscale Demo 1",desc:"2x-AniScale-compact, 2x super-resolution model, good details, low smearing and sharpening"}]},null,8,["items"]),i(r,{items:[{first:"/Statics/UserGuide/SrCompare/aniscale_1_in.png",second:"/Statics/UserGuide/SrCompare/aniscale_1_out.png",name:"Aniscale Demo 2",desc:"Same as above"}]},null,8,["items"]),i(r,{items:[{first:"/Statics/UserGuide/SrCompare/animevideo-v3_in.png",second:"/Statics/UserGuide/SrCompare/animevideo-v3_out.png",name:"ealesr-animevideov3-x2",desc:"Two-times super-resolution model, good details, slight smearing, medium sharpening"}]},null,8,["items"]),Z,$,e("p",null,[e("a",ee,[t("OpenModelDB"),i(a)]),t(" supports the model structure as shown in the following figure")]),te,t(" Among them, the ones compatible with SVFI are Compact, SPAN, ATD, ONNX (TensorRT). "),ie,e("p",null,[t("You can also add other supported super-resolution models such as "),e("a",ne,[t("AnimeJanai"),i(a)]),t(".")]),ae,i(r,{items:[{first:"/Statics/UserGuide/SrCompare/inpaint_in.png",second:"/Statics/UserGuide/SrCompare/inpaint_out.png",name:"InPaint Watermark Removal Effect Demonstration",desc:"It is recommended that the sequence length is greater than 30"}]},null,8,["items"]),oe,e("div",re,[se,e("p",null,[t("The following codecs need to purchase the "),e("a",le,[t("Professional DLC"),i(a)])])]),de,e("div",he,[ce,e("ul",null,[pe,ue,e("li",null,[t("Hard encoding gives priority to NVENC. In the NVIDIA GPU hard encoding preset (you can hover the mouse to view the description), you can query the hard encoding preset level of your own graphics card on the "),e("a",me,[t("driver official website"),i(a)]),t(". Generally, 20 and 30 series are 7th+.")]),fe,ge])]),be,e("div",ve,[ye,e("p",null,[t("This feature requires the purchase of the "),e("a",we,[t("Professional DLC"),i(a)]),t(".")])]),_e,xe,ke,e("div",Se,[Te,e("p",null,[t("This feature requires the purchase of the "),e("a",Ce,[t("Professional DLC"),i(a)]),t(".")])]),Ie,e("div",Ae,[Re,e("p",null,[t("This feature requires the purchase of the "),e("a",qe,[t("Professional DLC"),i(a)]),t(".")])]),Fe,e("div",Pe,[Ue,e("p",null,[t("This feature requires the purchase of the "),e("a",De,[t("Professional DLC"),i(a)]),t(".")])]),Ve,Ne,e("div",Ee,[Ge,e("p",null,[t("This feature requires the purchase of the "),e("a",Me,[t("Professional DLC"),i(a)]),t(".")])]),ze,e("blockquote",null,[i(o,{text:"Example"}),t(" Custom compression parameters for CPU h265 compression:"),We]),Oe,e("div",je,[Le,e("p",null,[t("This feature requires the purchase of the "),e("a",He,[t("Professional DLC"),i(a)]),t(".")])]),Be,e("div",Ke,[Qe,e("p",null,[t("For anime materials, please try to enable "),i(s,{to:"/en/pages/advanced-settings/#video-smoothness-optimization"},{default:d(()=>[t("Forward Weighting")]),_:1}),t(" in the "),Ye,t(" of the "),Je,t(" as much as possible.")]),Xe,Ze]),$e,e("div",et,[tt,e("p",null,[it,t(", "),nt,t(", "),at,t(" only support algorithms and models that can interpolate frames at any time"),ot,t(" If you are not sure whether your video is one frame per two or one frame per three, please check "),e("a",rt,[t("One Frame per N Introduction"),i(a)]),t(".")]),st]),lt,e("p",null,[t("These algorithms perform differently on different materials, and the algorithms and models for live-action and animation materials are respectively shown in "),i(s,{to:"/en/pages/svfi-presets/"},{default:d(()=>[t("Presets")]),_:1}),t(" and the following introduction")]),dt,e("div",ht,[ct,e("p",null,[t("Models with the "),pt,t(" prefix use "),e("a",ut,[t("ncnn"),i(a)]),t(" as the forward reasoning framework, which is compatible with NVIDIA GPUs and AMD GPUs, and models without this prefix cannot be used for AMD GPUs and core displays.")])]),mt,e("div",ft,[gt,e("p",null,[t("This feature requires the purchase of the "),e("a",bt,[t("Professional DLC"),i(a)]),t(".")])]),vt,e("div",yt,[wt,e("p",null,[t("This feature requires the purchase of the "),e("a",_t,[t("Professional DLC"),i(a)]),t(".")])]),xt,kt,e("div",St,[Tt,e("p",null,[t("This feature requires the purchase of the "),e("a",Ct,[t("Professional DLC"),i(a)]),t(".")])]),It,e("ul",null,[e("li",null,[e("p",null,[t("Fill in the complete path of the video ("),i(o,{text:"Example"}),t(),At,t(")")])]),e("li",null,[e("p",null,[t("Fill in the audio path of the video ("),i(o,{text:"Example"}),t(),Rt,t("), or use a video to input audio ("),i(o,{text:"Example"}),t(),qt,t(")")])]),e("li",null,[e("p",null,[t("Output video path ("),i(o,{text:"Example"}),t(),Ft,t(")")])]),Pt]),Ut,e("p",null,[t("Export the settings information as an ini file, which can be shared with other users to contribute their settings. The usage method is to directly drag it into the software, and it will prompt that the preset has been successfully applied."),Dt,t(" See "),i(s,{to:"/en/pages/useful-tips/"},{default:d(()=>[t("Usage Tips")]),_:1}),t(" for detailed usage.")]),Vt,e("p",null,[t("The software will default to quantize the model for 750 rounds, and the number of quantizations can be adjusted in "),i(s,{to:"/en/pages/other-advanced-settings/"},{default:d(()=>[t("Advanced Related Settings")]),_:1}),t(". The processing time is long, and the acceleration may not be obvious on some devices.")]),Nt])}const jt=v(D,[["render",Et],["__file","index.html.vue"]]);export{jt as default};
